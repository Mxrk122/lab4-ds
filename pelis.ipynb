{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias necesarias\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 50000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 secuenciar y rellenar criticas para que sean uniformes\n",
    "max_sequence_length = 200  # Longitud máxima de la secuencia de palabras\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_sequence_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_sequence_length)\n",
    "\n",
    "# 2 crear una funcion para extraer caracteristicas de una critica\n",
    "# recibe una critica como parametro\n",
    "def get_review_features(review_text):\n",
    "    \n",
    "    # Diccionario que mapea índices de palabras a palabras\n",
    "    word_index_reverse = {index: word for word, index in imdb.get_word_index().items()}\n",
    "\n",
    "    # Convierte la secuencia numérica en texto claro\n",
    "    review_text = \" \".join([word_index_reverse.get(index, \"\") for index in review_text])\n",
    "    # Calcular la longitud de la crítica\n",
    "    review_length = len(review_text.split())\n",
    "\n",
    "    # crear nuestro indice de palabras\n",
    "    positive_words = [\"good\", \"excellent\", \"outstanding\", \"fantastic\", \"awesome\", \"funny\", \"hilarious\", \"cool\"]\n",
    "    negative_words = [\"bad\", \"poor\", \"terrible\", \"awful\", \"horrible\", \"boring\", \"sleepy\", \"fool\", \"silly\", \"goofy\"]\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    # Contar palabras positivas y negativas\n",
    "    for word in review_text.split():\n",
    "        if word in positive_words:\n",
    "            positive_count += 1\n",
    "        if word in negative_words:\n",
    "            negative_count += 1\n",
    "    # Calcular la proporción de palabras positivas/negativas\n",
    "    if (positive_count + negative_count) > 0:\n",
    "        positivity_ratio = positive_count / (positive_count + negative_count)\n",
    "    else:\n",
    "        positivity_ratio = 0.0\n",
    "\n",
    "    return review_length, positivity_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar features extra\n",
    "rl = []\n",
    "pr = []\n",
    "\n",
    "for review in x_train:\n",
    "    uwu, owo = get_review_features(review)\n",
    "\n",
    "    rl.append(uwu)\n",
    "    pr.append(owo)\n",
    "\n",
    "# Convierte las listas de características en arreglos numpy\n",
    "feature_1_train = np.array(rl)\n",
    "feature_2_train = np.array(pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200 189 141 ... 184 150 153]\n"
     ]
    }
   ],
   "source": [
    "print(feature_1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = []\n",
    "pr = []\n",
    "\n",
    "for review in x_test:\n",
    "    uwu, owo = get_review_features(review)\n",
    "\n",
    "    rl.append(uwu)\n",
    "    pr.append(owo)\n",
    "\n",
    "# Convierte las listas de características en arreglos numpy\n",
    "feature_1_test = np.array(rl)\n",
    "feature_2_test = np.array(pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Preparar las características adicionales\n",
    "new_features_train = np.stack([feature_1_train, feature_2_train], axis=1)\n",
    "new_features_test = np.stack([feature_1_test, feature_2_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 70s 109ms/step - loss: 0.7727 - accuracy: 0.6520 - val_loss: 0.4208 - val_accuracy: 0.8230\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 68s 109ms/step - loss: 0.3202 - accuracy: 0.8675 - val_loss: 0.4067 - val_accuracy: 0.8258\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 69s 110ms/step - loss: 0.1737 - accuracy: 0.9369 - val_loss: 0.3700 - val_accuracy: 0.8556\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 68s 108ms/step - loss: 0.0899 - accuracy: 0.9712 - val_loss: 0.4891 - val_accuracy: 0.8514\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 69s 111ms/step - loss: 0.0506 - accuracy: 0.9844 - val_loss: 0.5082 - val_accuracy: 0.8548\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 0.0321 - accuracy: 0.9912 - val_loss: 0.5856 - val_accuracy: 0.8490\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 0.0278 - accuracy: 0.9916 - val_loss: 0.6457 - val_accuracy: 0.8530\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 69s 110ms/step - loss: 0.0209 - accuracy: 0.9944 - val_loss: 0.7438 - val_accuracy: 0.8522\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 0.0125 - accuracy: 0.9968 - val_loss: 0.7558 - val_accuracy: 0.8428\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.8392 - val_accuracy: 0.8426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eed891b390>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definición de la estructura del modelo\n",
    "\n",
    "# Entrada para la secuencia de palabras\n",
    "word_input = Input(shape=(200,), dtype='int32', name='word_input')\n",
    "\n",
    "# Capa de incrustación (embedding) para convertir palabras en vectores\n",
    "word_embedding = Embedding(input_dim=50000, output_dim=128, input_length=200)(word_input)\n",
    "\n",
    "# Capa LSTM para procesar la secuencia de palabras\n",
    "lstm_out = LSTM(32)(word_embedding)\n",
    "\n",
    "# Entrada para las características adicionales\n",
    "additional_input = Input(shape=(2,), name='additional_input')\n",
    "\n",
    "# Combinar las salidas de la capa LSTM y las características adicionales\n",
    "merged = concatenate([lstm_out, additional_input])\n",
    "\n",
    "# Capa densa de salida con activación sigmoide para clasificación binaria\n",
    "output = Dense(1, activation='sigmoid', name='output')(merged)\n",
    "\n",
    "# Crear el modelo\n",
    "model_simple = Model(inputs=[word_input, additional_input], outputs=[output])\n",
    "\n",
    "# Compilar el modelo con optimizador 'adam' y función de pérdida 'binary_crossentropy'\n",
    "model_simple.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model_simple.fit([x_train, new_features_train], y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9097 - accuracy: 0.8319\n",
      "Test loss: 0.9097073674201965\n",
      "Test accuracy: 0.8318799734115601\n"
     ]
    }
   ],
   "source": [
    "score = model_simple.evaluate([x_test, new_features_test], y_test, batch_size=32)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura mas compleja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrada para la secuencia de palabras\n",
    "word_input = Input(shape=(200,), dtype='int32', name='word_input')\n",
    "\n",
    "# Capa de incrustación para representar palabras como vectores densos\n",
    "word_embedding = Embedding(input_dim=50000, output_dim=128, input_length=200)(word_input)\n",
    "\n",
    "# Capa LSTM para procesar la secuencia de palabras con regularización Dropout\n",
    "lstm_out1 = LSTM(64, return_sequences=True)(word_embedding)  # LSTM con retorno de secuencias\n",
    "dropout1 = Dropout(0.4)(lstm_out1)  # Dropout para regularización\n",
    "\n",
    "# Capa LSTM adicional\n",
    "lstm_out2 = LSTM(32)(dropout1)  # LSTM sin retorno de secuencias\n",
    "dropout2 = Dropout(0.4)(lstm_out2)  # Dropout para regularización\n",
    "\n",
    "# Entrada para las características adicionales (aquí asumimos 3 características)\n",
    "additional_input = Input(shape=(2,), name='additional_input')\n",
    "\n",
    "# Combinar las salidas de las capas LSTM y las características adicionales\n",
    "merged = concatenate([dropout2, additional_input])\n",
    "\n",
    "# Capas densas adicionales para procesar la información combinada\n",
    "dense1 = Dense(32, activation='relu')(merged)\n",
    "dense2 = Dense(16, activation='relu')(dense1)\n",
    "\n",
    "# Capa densa de salida con activación sigmoide para clasificación binaria\n",
    "output = Dense(1, activation='sigmoid', name='output')(dense2)\n",
    "\n",
    "# Crear y compilar el modelo\n",
    "model_complex = Model(inputs=[word_input, additional_input], outputs=[output])\n",
    "model_complex.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 91s 141ms/step - loss: 0.4778 - accuracy: 0.7775 - val_loss: 0.3359 - val_accuracy: 0.8598\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 87s 139ms/step - loss: 0.2050 - accuracy: 0.9238 - val_loss: 0.3072 - val_accuracy: 0.8716\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 87s 139ms/step - loss: 0.1075 - accuracy: 0.9638 - val_loss: 0.5119 - val_accuracy: 0.8532\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 87s 140ms/step - loss: 0.0564 - accuracy: 0.9816 - val_loss: 0.5767 - val_accuracy: 0.8628\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 87s 139ms/step - loss: 0.0591 - accuracy: 0.9803 - val_loss: 0.4722 - val_accuracy: 0.8456\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 93s 148ms/step - loss: 0.0388 - accuracy: 0.9886 - val_loss: 0.6287 - val_accuracy: 0.8558\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 0.7722 - val_accuracy: 0.8558\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 87s 139ms/step - loss: 0.0278 - accuracy: 0.9916 - val_loss: 0.7128 - val_accuracy: 0.8586\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 87s 140ms/step - loss: 0.0315 - accuracy: 0.9903 - val_loss: 0.8355 - val_accuracy: 0.8602\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 86s 138ms/step - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.8844 - val_accuracy: 0.8548\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.9552 - accuracy: 0.8404\n",
      "Test loss: 0.9551502466201782\n",
      "Test accuracy: 0.840399980545044\n"
     ]
    }
   ],
   "source": [
    "# Entrenar y evaluar el modelo complejo\n",
    "history_complex = model_complex.fit([x_train, new_features_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "score_complex = model_complex.evaluate([x_test, new_features_test], y_test, batch_size=32)\n",
    "print('Test loss:', score_complex[0])\n",
    "print('Test accuracy:', score_complex[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 22s 28ms/step - loss: 0.9552 - accuracy: 0.8404\n",
      "Test loss: 0.9551502466201782\n",
      "Test accuracy: 0.840399980545044\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
